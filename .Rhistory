title='Total Sales vs. Days Till Holiday',
x='Days Till Holiday',
y='Total Sales'
) +
theme_minimal()
holiday_plot
google_store <- google_store %>%
left_join(holiday_group, "days_till_holiday")
google_store
logistic_regression <- function(
df,
response_var,
factor_limit = 5,
numeric_features = NA,
categorical_features,
theta = 0.5){
df = na.omit(df)
# First I need to take an aggressive approach to cleaning the data to avoid
# mismatches. For this model I will only consider the top n most common
# features in order to adjust speed and accuracy.
message("Trimming Features")
# Generally I would define this out of the function.
# It's a bit more readable here though.
top_n_values <- function(column, n) {
names(head(sort(table(column), decreasing = TRUE), n))}
# Looping through each row and removing categorical
# features not in the top n most common. This simplifies the dataset to allow
# reasonable compute times.
for (col in categorical_features) {
top_values <- top_n_values(df[[col]], factor_limit)
df <- df[df[[col]] %in% top_values, ]}
#df <- factor(df[[col]], levels = top_values)
sprintf("The dataset has been reduced to %s rows", nrow(df)) %>% message()
# Need to make sure numeric features isn't NA.
# Then standardize them.
if (!anyNA(numeric_features)){
for (col in numeric_features){
df[[col]] <- scale(df[[col]])
}}
message("Splitting Dataset")
# Split the dataset into test-train sets
train_indices <- caret::createDataPartition(df[[response_var]], p = 0.8, list = FALSE)
use_columns <- c(numeric_features, categorical_features, response_var) %>% na.omit()
log_reg_train <- df[train_indices, use_columns]
log_reg_test <- df[-train_indices, use_columns]
message("Building Model")
# Put together model formula
model_formula <- as.formula(paste(response_var, "~ ."))
# Build the model
model <- glm(
formula = model_formula,
family = binomial(link = "logit"),
data = log_reg_train)
# Predict values for test & train set
message("Predicting Values")
# Helper function for simplicity
predict_values <- function(df, theta=0.5, model){
df$prediction <- predict(
model, newdata = df, type = "response")
df$prediction <- ifelse(df$prediction > theta, 1, 0)
return(df)}
log_reg_train <- predict_values(log_reg_train, theta, model)
log_reg_test <- predict_values(log_reg_test, theta, model)
message("Determining results")
results <- function(df, data_type){
accuracy <- sum(df[[response_var]] == df$prediction, na.rm=TRUE) / nrow(df)
confusion_matrix_count <- table(df[[response_var]], df$prediction)
confusion_matrix <- prop.table(confusion_matrix_count, margin = 1)
sprintf("Data Type: %s", data_type) %>% print()
sprintf("The accuracy was %s", percent(accuracy)) %>% print()
tp <- confusion_matrix[2, 2]
tn <- confusion_matrix[1, 1]
sprintf("The TP rate is %s", percent(tp)) %>% print()
sprintf("The TN rate is %s", percent(tn)) %>% print()
return(list(accuracy, tp, tn))
}
train_results <- results(log_reg_train, "Train")
test_results <- results(log_reg_test, "Test")
return(list(model, train_results, test_results))
}
categorical_features <- c(
"traffic_source", "traffic_medium", "month",
"operating_system", "user_country", "browser")
numeric_features <- c(
"total_time_on_site", "total_pageviews", "previous_visit_count",
"previous_purchase_count", "is_weekend", "total_sales_before_holiday")
purchase_reg <- logistic_regression(
df = google_store,
response_var = "purchase",
factor_limit = 5,
numeric_features = numeric_features,
categorical_features = categorical_features,
theta = 0.05 # Eyeballed optimal theta
)
purchase_model <- purchase_reg[[1]]
#ggstats::ggcoef_model(purchase_model,
#                      exponentiate = TRUE,
#                      ci_method="wald")+
ggstatsplot::ggcoefstats(
x = purchase_model,
exponentiate = TRUE,
conf.level = 0.95,
ci.method = "wald"  # Specify method if needed
)+
theme_classic(base_size = 5)
install.packages('rlang')
install.packages("rlang")
purchase_model <- purchase_reg[[1]]
#ggstats::ggcoef_model(purchase_model,
#                      exponentiate = TRUE,
#                      ci_method="wald")+
ggstatsplot::ggcoefstats(
x = purchase_model,
exponentiate = TRUE,
conf.level = 0.95,
ci.method = "wald"  # Specify method if needed
)+
theme_classic(base_size = 5)
library(tidyverse)
library(broom)
library(scales)
library(lubridate)
library(ggplot2)
google_store <- read_csv("data/google_store.csv")
daily_stats <- read_csv("data/daily_stats.csv")
library(tidyverse)
library(broom)
library(scales)
library(lubridate)
library(ggplot2)
library(rlang)
google_store <- read_csv("data/google_store.csv")
daily_stats <- read_csv("data/daily_stats.csv")
google_store <- google_store %>%
mutate(
purchase = ifelse(is.na(revenue) | revenue == 0, 0, 1),
bounces = replace_na(bounces, 0),
revenue = replace_na(revenue, 0),
total_time_on_site = replace_na(total_time_on_site, 0),
transactions = replace_na(transactions, 0),
session_quality = replace_na(session_quality, 50)
)
which(colSums(is.na(google_store)) > 0)
# Modification Using Lubridate
google_store <- google_store %>%
mutate(
date = ymd(google_store$date),
weekday = wday(date, label = TRUE, abbr =  TRUE),
is_weekend = wday(date) %in% c(1, 7),
month = month(date, label = TRUE, abbr = TRUE)
)
# Joining with a dataset of holidays and days until the next holiday
google_store <- left_join(
google_store,
daily_stats[,c("date", "user_country", "is_holiday", "days_till_holiday")],
by=c("date", "user_country")
)
google_store <- google_store %>%
group_by(fullVisitorId) %>%
summarize(
previous_visit_count = n(), previous_purchase_count = sum(purchase)
) %>%
right_join(google_store, 'fullVisitorId')
holiday_group <- daily_stats %>%
group_by(days_till_holiday) %>%
summarize(total_sales_before_holiday = sum(total_sales)) %>%
arrange(desc(total_sales_before_holiday)) %>%
na.omit()
holiday_plot <- ggplot(holiday_group, aes(x=days_till_holiday, y=total_sales_before_holiday)) +
geom_line() +
scale_x_reverse(limits = c(70, 0)) +
scale_y_continuous(limits = c(0, 600)) +
labs(
title='Total Sales vs. Days Till Holiday',
x='Days Till Holiday',
y='Total Sales'
) +
theme_minimal()
holiday_plot
google_store <- google_store %>%
left_join(holiday_group, "days_till_holiday")
google_store
logistic_regression <- function(
df,
response_var,
factor_limit = 5,
numeric_features = NA,
categorical_features,
theta = 0.5){
df = na.omit(df)
# First I need to take an aggressive approach to cleaning the data to avoid
# mismatches. For this model I will only consider the top n most common
# features in order to adjust speed and accuracy.
message("Trimming Features")
# Generally I would define this out of the function.
# It's a bit more readable here though.
top_n_values <- function(column, n) {
names(head(sort(table(column), decreasing = TRUE), n))}
# Looping through each row and removing categorical
# features not in the top n most common. This simplifies the dataset to allow
# reasonable compute times.
for (col in categorical_features) {
top_values <- top_n_values(df[[col]], factor_limit)
df <- df[df[[col]] %in% top_values, ]}
#df <- factor(df[[col]], levels = top_values)
sprintf("The dataset has been reduced to %s rows", nrow(df)) %>% message()
# Need to make sure numeric features isn't NA.
# Then standardize them.
if (!anyNA(numeric_features)){
for (col in numeric_features){
df[[col]] <- scale(df[[col]])
}}
message("Splitting Dataset")
# Split the dataset into test-train sets
train_indices <- caret::createDataPartition(df[[response_var]], p = 0.8, list = FALSE)
use_columns <- c(numeric_features, categorical_features, response_var) %>% na.omit()
log_reg_train <- df[train_indices, use_columns]
log_reg_test <- df[-train_indices, use_columns]
message("Building Model")
# Put together model formula
model_formula <- as.formula(paste(response_var, "~ ."))
# Build the model
model <- glm(
formula = model_formula,
family = binomial(link = "logit"),
data = log_reg_train)
# Predict values for test & train set
message("Predicting Values")
# Helper function for simplicity
predict_values <- function(df, theta=0.5, model){
df$prediction <- predict(
model, newdata = df, type = "response")
df$prediction <- ifelse(df$prediction > theta, 1, 0)
return(df)}
log_reg_train <- predict_values(log_reg_train, theta, model)
log_reg_test <- predict_values(log_reg_test, theta, model)
message("Determining results")
results <- function(df, data_type){
accuracy <- sum(df[[response_var]] == df$prediction, na.rm=TRUE) / nrow(df)
confusion_matrix_count <- table(df[[response_var]], df$prediction)
confusion_matrix <- prop.table(confusion_matrix_count, margin = 1)
sprintf("Data Type: %s", data_type) %>% print()
sprintf("The accuracy was %s", percent(accuracy)) %>% print()
tp <- confusion_matrix[2, 2]
tn <- confusion_matrix[1, 1]
sprintf("The TP rate is %s", percent(tp)) %>% print()
sprintf("The TN rate is %s", percent(tn)) %>% print()
return(list(accuracy, tp, tn))
}
train_results <- results(log_reg_train, "Train")
test_results <- results(log_reg_test, "Test")
return(list(model, train_results, test_results))
}
categorical_features <- c(
"traffic_source", "traffic_medium", "month",
"operating_system", "user_country", "browser")
numeric_features <- c(
"total_time_on_site", "total_pageviews", "previous_visit_count",
"previous_purchase_count", "is_weekend", "total_sales_before_holiday")
purchase_reg <- logistic_regression(
df = google_store,
response_var = "purchase",
factor_limit = 5,
numeric_features = numeric_features,
categorical_features = categorical_features,
theta = 0.05 # Eyeballed optimal theta
)
purchase_model <- purchase_reg[[1]]
#ggstats::ggcoef_model(purchase_model,
#                      exponentiate = TRUE,
#                      ci_method="wald")+
ggstatsplot::ggcoefstats(
x = purchase_model,
exponentiate = TRUE,
conf.level = 0.95
)+
theme_classic(base_size = 5)
gc()
library(tidyverse)
library(broom)
library(scales)
library(lubridate)
library(ggplot2)
library(rlang)
google_store <- read_csv("data/google_store.csv")
daily_stats <- read_csv("data/daily_stats.csv")
google_store <- google_store %>%
mutate(
purchase = ifelse(is.na(revenue) | revenue == 0, 0, 1),
bounces = replace_na(bounces, 0),
revenue = replace_na(revenue, 0),
total_time_on_site = replace_na(total_time_on_site, 0),
transactions = replace_na(transactions, 0),
session_quality = replace_na(session_quality, 50)
)
which(colSums(is.na(google_store)) > 0)
# Modification Using Lubridate
google_store <- google_store %>%
mutate(
date = ymd(google_store$date),
weekday = wday(date, label = TRUE, abbr =  TRUE),
is_weekend = wday(date) %in% c(1, 7),
month = month(date, label = TRUE, abbr = TRUE)
)
# Joining with a dataset of holidays and days until the next holiday
google_store <- left_join(
google_store,
daily_stats[,c("date", "user_country", "is_holiday", "days_till_holiday")],
by=c("date", "user_country")
)
google_store <- google_store %>%
group_by(fullVisitorId) %>%
summarize(
previous_visit_count = n(), previous_purchase_count = sum(purchase)
) %>%
right_join(google_store, 'fullVisitorId')
arrange(google_store, desc(previous_visit_count))
holiday_group <- daily_stats %>%
group_by(days_till_holiday) %>%
summarize(total_sales_before_holiday = sum(total_sales)) %>%
arrange(desc(total_sales_before_holiday)) %>%
na.omit()
holiday_plot <- ggplot(holiday_group, aes(x=days_till_holiday, y=total_sales_before_holiday)) +
geom_line() +
scale_x_reverse(limits = c(70, 0)) +
scale_y_continuous(limits = c(0, 600)) +
labs(
title='Total Sales vs. Days Till Holiday',
x='Days Till Holiday',
y='Total Sales'
) +
theme_minimal()
holiday_plot
google_store <- google_store %>%
left_join(holiday_group, "days_till_holiday")
google_store
logistic_regression <- function(
df,
response_var,
factor_limit = 5,
numeric_features = NA,
categorical_features,
theta = 0.5){
df = na.omit(df)
# First I need to take an aggressive approach to cleaning the data to avoid
# mismatches. For this model I will only consider the top n most common
# features in order to adjust speed and accuracy.
message("Trimming Features")
# Generally I would define this out of the function.
# It's a bit more readable here though.
top_n_values <- function(column, n) {
names(head(sort(table(column), decreasing = TRUE), n))}
# Looping through each row and removing categorical
# features not in the top n most common. This simplifies the dataset to allow
# reasonable compute times.
for (col in categorical_features) {
top_values <- top_n_values(df[[col]], factor_limit)
df <- df[df[[col]] %in% top_values, ]}
#df <- factor(df[[col]], levels = top_values)
sprintf("The dataset has been reduced to %s rows", nrow(df)) %>% message()
# Need to make sure numeric features isn't NA.
# Then standardize them.
if (!anyNA(numeric_features)){
for (col in numeric_features){
df[[col]] <- scale(df[[col]])
}}
message("Splitting Dataset")
# Split the dataset into test-train sets
train_indices <- caret::createDataPartition(df[[response_var]], p = 0.8, list = FALSE)
use_columns <- c(numeric_features, categorical_features, response_var) %>% na.omit()
log_reg_train <- df[train_indices, use_columns]
log_reg_test <- df[-train_indices, use_columns]
message("Building Model")
# Put together model formula
model_formula <- as.formula(paste(response_var, "~ ."))
# Build the model
model <- glm(
formula = model_formula,
family = binomial(link = "logit"),
data = log_reg_train)
# Predict values for test & train set
message("Predicting Values")
# Helper function for simplicity
predict_values <- function(df, theta=0.5, model){
df$prediction <- predict(
model, newdata = df, type = "response")
df$prediction <- ifelse(df$prediction > theta, 1, 0)
return(df)}
log_reg_train <- predict_values(log_reg_train, theta, model)
log_reg_test <- predict_values(log_reg_test, theta, model)
message("Determining results")
results <- function(df, data_type){
accuracy <- sum(df[[response_var]] == df$prediction, na.rm=TRUE) / nrow(df)
confusion_matrix_count <- table(df[[response_var]], df$prediction)
confusion_matrix <- prop.table(confusion_matrix_count, margin = 1)
sprintf("Data Type: %s", data_type) %>% print()
sprintf("The accuracy was %s", percent(accuracy)) %>% print()
tp <- confusion_matrix[2, 2]
tn <- confusion_matrix[1, 1]
sprintf("The TP rate is %s", percent(tp)) %>% print()
sprintf("The TN rate is %s", percent(tn)) %>% print()
return(list(accuracy, tp, tn))
}
train_results <- results(log_reg_train, "Train")
test_results <- results(log_reg_test, "Test")
return(list(model, train_results, test_results))
}
categorical_features <- c(
"traffic_source", "traffic_medium", "month",
"operating_system", "user_country", "browser")
numeric_features <- c(
"total_time_on_site", "total_pageviews", "previous_visit_count",
"previous_purchase_count", "is_weekend", "total_sales_before_holiday")
purchase_reg <- logistic_regression(
df = google_store,
response_var = "purchase",
factor_limit = 5,
numeric_features = numeric_features,
categorical_features = categorical_features,
theta = 0.05 # Eyeballed optimal theta
)
purchase_model <- purchase_reg[[1]]
#ggstats::ggcoef_model(purchase_model,
#                      exponentiate = TRUE,
#                      ci_method="wald")+
ggstatsplot::ggcoefstats(
x = purchase_model,
exponentiate = TRUE,
conf.level = 0.95
)+
theme_classic(base_size = 5)
install.packages(‘rlang’)
# Modification Using Lubridate
google_store <- google_store %>%
mutate(
date = ymd(google_store$date),
weekday = wday(date, label = TRUE, abbr =  TRUE),
is_weekend = wday(date) %in% c(1, 7),
month = month(date, label = TRUE, abbr = TRUE)
)
install.packages("rlang")
# Joining with a dataset of holidays and days until the next holiday
google_store <- left_join(
google_store,
daily_stats[,c("date", "user_country", "is_holiday", "days_till_holiday")],
by=c("date", "user_country")
)
google_store <- google_store %>%
group_by(fullVisitorId) %>%
summarize(
previous_visit_count = n(), previous_purchase_count = sum(purchase)
) %>%
right_join(google_store, 'fullVisitorId')
remove.packages("rlang")
install.packages("rlang")
library(tidyverse)
library(broom)
library(scales)
library(lubridate)
library(ggplot2)
library(rlang)
google_store <- read_csv("data/google_store.csv")
daily_stats <- read_csv("data/daily_stats.csv")
google_store <- google_store %>%
mutate(
purchase = ifelse(is.na(revenue) | revenue == 0, 0, 1),
bounces = replace_na(bounces, 0),
revenue = replace_na(revenue, 0),
total_time_on_site = replace_na(total_time_on_site, 0),
transactions = replace_na(transactions, 0),
session_quality = replace_na(session_quality, 50)
)
which(colSums(is.na(google_store)) > 0)
# Modification Using Lubridate
google_store <- google_store %>%
mutate(
date = ymd(google_store$date),
weekday = wday(date, label = TRUE, abbr =  TRUE),
is_weekend = wday(date) %in% c(1, 7),
month = month(date, label = TRUE, abbr = TRUE)
)
# Joining with a dataset of holidays and days until the next holiday
google_store <- left_join(
google_store,
daily_stats[,c("date", "user_country", "is_holiday", "days_till_holiday")],
by=c("date", "user_country")
)
google_store <- google_store %>%
group_by(fullVisitorId) %>%
summarize(
previous_visit_count = n(), previous_purchase_count = sum(purchase)
) %>%
right_join(google_store, 'fullVisitorId')
holiday_group <- daily_stats %>%
group_by(days_till_holiday) %>%
summarize(total_sales_before_holiday = sum(total_sales)) %>%
arrange(desc(total_sales_before_holiday)) %>%
na.omit()
holiday_plot <- ggplot(holiday_group, aes(x=days_till_holiday, y=total_sales_before_holiday)) +
geom_line() +
scale_x_reverse(limits = c(70, 0)) +
scale_y_continuous(limits = c(0, 600)) +
labs(
title='Total Sales vs. Days Till Holiday',
x='Days Till Holiday',
y='Total Sales'
) +
theme_minimal()
holiday_plot
google_store <- google_store %>%
left_join(holiday_group, "days_till_holiday")
google_store
