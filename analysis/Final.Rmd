---
title: "Logistic Regression Final"
author: "Eli Andrae, "
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

# 1. Execuative Summary

Write Here:

# 2. Setup

## Load Required Libraries

```{r}
library(tidyverse)
library(broom)
library(scales)
library(lubridate)
library(ggplot2)
```

## Load Data

```{r}
google_store <- read_csv("../data/google_store.csv")
daily_stats <- read_csv("../data/daily_stats.csv")
```

First I will clear the NA values to make the data usable in a Regression

# Data Cleaning & Feature Engineering

-   Cleaning our NA Values and Creating The Purchase Variable

```{r}
google_store <- google_store %>% 
  mutate(
    purchase = ifelse(is.na(revenue) | revenue == 0, 0, 1),
    bounces = replace_na(bounces, 0),
    revenue = replace_na(revenue, 0),
    total_time_on_site = replace_na(total_time_on_site, 0),
    transactions = replace_na(transactions, 0),
    session_quality = replace_na(session_quality, 50),
    user_country = replace_na(user_country, "Not Observed")
  )
which(colSums(is.na(google_store)) > 0)
```

## Feature Engineering Using Date Functions

Now we will add some extra features to the data. Due to compute times
some of these have been excluded from the file and presaved as a CSV
file. Not every country and holiday is included, however, I tried my
best to include as many as possible using [date.nager
API](https://date.nager.at/api/v3/publicholidays/). View
`HolidayAnalysis.Rmd` for more info on how the holidays were accessed
and `days_till_holiday` was calculated.\*\*

```{r}
# Modification Using Lubridate
google_store <- google_store %>%
  mutate(
    date = ymd(google_store$date),
    weekday = wday(date, label = TRUE, abbr =  TRUE),
    is_weekend = wday(date) %in% c(1, 7),
    month = month(date, label = TRUE, abbr = TRUE)
  )

google_store$month <- factor(google_store$month, 
                             levels = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", 
                                        "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"), 
                             ordered = FALSE)
```

## Merging Holiday Data and Visior Data

```{r}
# Joining with a dataset of holidays and days until the next holiday
google_store <- left_join(
  google_store, 
  daily_stats[,c("date", "user_country", "is_holiday", "days_till_holiday")], 
  by=c("date", "user_country")
)

google_store <- google_store %>%
  group_by(fullVisitorId) %>%
  summarize(
    previous_visit_count = n(), previous_purchase_count = sum(purchase)
  ) %>%
  right_join(google_store, 'fullVisitorId')
```

# Finding Optimal Pre-Holiday Marketing Period

I'm thinking there might be an optimal period to market before holidays.

```{r}
holiday_group <- daily_stats %>%
  group_by(days_till_holiday) %>%
  summarize(total_sales_before_holiday = sum(total_sales)) %>%
  arrange(desc(total_sales_before_holiday)) %>%
  na.omit()

arrange(holiday_group, desc(total_sales_before_holiday))
```

The most common days to place an order before a holiday are 7, 6, and 4
days. It is useful to market much closer to a holiday then I may have
expected.

```{r}
holiday_plot <- ggplot(holiday_group, aes(x=days_till_holiday, y=total_sales_before_holiday)) +
  geom_line() +
  scale_x_reverse(limits = c(70, 0)) +
  scale_y_continuous(limits = c(0, 600)) +
    labs(
        title='Total Sales vs. Days Till Holiday',
        x='Days Till Holiday',
        y='Total Sales'
    ) +
    theme_bw()
holiday_plot
```

-   Just as I suspected, as it gets closer to a holiday, sales go up
    around the world.
-   In order to handle this in the logistic regression model, we
    attempted an inverse transformation but yeilded minimal results. We
    found the more effective method for the model was to find total
    country sales for each number of days before the holiday. The number
    of sales for that day can then be used as a weight in the logistic
    regression model.

```{r}
google_store <- google_store %>%
  left_join(holiday_group, "days_till_holiday") 
google_store
```

# Looking at Linear Correlation

```{r}
summary(google_store)
```

# Defining Logestic Regression model

-   In this section I will build a logistic regression model build for
    our data set to determine likelihood of a bounce pre visiting, and
    likelihood of a purchase of a user on the site. As a marketer, this
    allows us to determine if it is worthwhile advertising to a user,
    (if they will invest time on the site) based on the way they found
    the site, and purchase likelihood allows us to start preparing for
    an order before it's made, and decide if it's worthwhile to send a
    follow up message if the order is not placed but the user is likely
    to (TP). Or if the user is not worthwhile advertising to in general.

```{r}
logistic_regression <- function(
    df, 
    response_var, 
    factor_limit = 5, 
    numeric_features = NA, 
    categorical_features, 
    theta = 0.5){
  
  df <- df %>% 
    filter(traffic_source != "analytics.google.com")
  
  # Make sure there's no columns with straggling NA values.
  df = na.omit(df)
  # Custom filter list
  df <- df %>% filter()
  # First I need to take an aggressive approach to cleaning the data to avoid 
  # mismatches. For this model I will only consider the top n most common 
  # features in order to adjust speed and accuracy.
  message("Trimming Features")
  # Generally I would define this out of the function. 
  # It's a bit more readable here though.
  top_n_values <- function(column, n) {
    names(head(sort(table(column), decreasing = TRUE), n))}
  
  # Looping through each row and removing categorical
  # features not in the top n most common. This simplifies the dataset to allow
  # reasonable compute times.
  for (col in categorical_features) {  
    top_values <- top_n_values(df[[col]], factor_limit)
    df <- df[df[[col]] %in% top_values, ]}
    #df <- factor(df[[col]], levels = top_values)
  sprintf("The dataset has been reduced to %s rows", nrow(df)) %>% message()
  
  # Need to make sure numeric features isn't NA. 
  # Then standardize them.
  if (!anyNA(numeric_features)){
    for (col in numeric_features){
      df[[col]] <- scale(df[[col]])
    }}
  
  message("Splitting Dataset")
  # Split the dataset into test-train sets
  train_indices <- caret::createDataPartition(df[[response_var]], p = 0.8, list = FALSE)
  
  use_columns <- c(numeric_features, categorical_features, response_var) %>% na.omit()
  
  log_reg_train <- df[train_indices, use_columns]
  log_reg_test <- df[-train_indices, use_columns]
  
  
  message("Building Model")
  
  # Put together model formula
  model_formula <- as.formula(paste(response_var, "~ ."))
  # Build the model
  model <- glm(
    formula = model_formula,
    family = binomial(link = "logit"),
    data = log_reg_train) 
   
  # Predict values for test & train set
  message("Predicting Values")
  # Helper function for simplicity
  predict_values <- function(df, theta=0.5, model){
    df$prediction <- predict(
      model, newdata = df, type = "response")
    df$prediction <- ifelse(df$prediction > theta, 1, 0)
    return(df)}
  
  log_reg_train <- predict_values(log_reg_train, theta, model)
  log_reg_test <- predict_values(log_reg_test, theta, model)
  
  message("Determining results")
  results <- function(df, data_type){
    accuracy <- sum(df[[response_var]] == df$prediction, na.rm=TRUE) / nrow(df)
    confusion_matrix_count <- table(df[[response_var]], df$prediction)
    confusion_matrix <- prop.table(confusion_matrix_count, margin = 1)
    sprintf("Data Type: %s", data_type) %>% print()
    sprintf("The accuracy was %s", percent(accuracy)) %>% print()
    tp <- confusion_matrix[2, 2]
    tn <- confusion_matrix[1, 1]
    sprintf("The TP rate is %s", percent(tp)) %>% print()
    sprintf("The TN rate is %s", percent(tn)) %>% print()
    return(list(accuracy, tp, tn))
  }
  train_results <- results(log_reg_train, "Train")
  test_results <- results(log_reg_test, "Test")
  return(list(model, train_results, test_results))
}
```

```{r}
categorical_features <- c(
  "traffic_source", "traffic_medium", 
  "operating_system", "browser", "user_country")

numeric_features <- c(
  "total_time_on_site", "total_pageviews", "previous_visit_count", 
  "previous_purchase_count", "total_sales_before_holiday")


feature_1 <- 
filter_features <- 

purchase_reg <- logistic_regression(
  df = google_store,
  response_var = "purchase",
  factor_limit = 5,
  numeric_features = numeric_features,
  categorical_features = categorical_features,
  theta = 0.05 # Eyeballed optimal theta
)


```

Purchase Model Results: - Ended up with extremely high accuracy and TN
rate. Theta can be adjusted in small increments(+-0.01) to increase TP
or NT rate. - I attempted a ridge regression model. I was able to avoid
0 or 1 probability error, however, I lost the modularity of the model.
Did not have time to improve upon.

```{r}

purchase_model <- purchase_reg[[1]]

purchase_plot <- ggstats::ggcoef_model(purchase_model,
                      exponentiate = TRUE)+
  theme_classic(base_size = 6) + 
  scale_x_continuous(
    labels = scales::number_format(accuracy = 1)
) +
  labs(
    title = "Coeficient Plot for Odds of Purchase Logistic Regression")

ggsave("purchase_model_plot.png",
       plot = last_plot(), 
       width = 8,
       height = 6)
```

Features to Keep: - Time on site - Page views - Remove: - Traffic
Source - 'analytics.google.com' - - 'Vietnam' With these results, I
might try to apply this model to a linear regression plot. With revenue
for guessed purchase compared to revenue for assumed not purchase.

High TN Results: - Reduced marketing spend if targeting ideal potential
buyers. - only focus on visitors more likely to purchase - can exclude
non-buyers from campaigns or personalized offers - Fraud or Bot
Detection. - If bot detection is an issue this can eliminate false
positives and reduce revenue loss for sales that won't go through.

```{r}
categorical_features <- c(
  "traffic_source", "operating_system", "browser", "month")

numeric_features <- c("total_sales_before_holiday", 
                      "previous_visit_count", "previous_purchase_count")

bounce_reg <- logistic_regression(
  df = google_store,
  response_var = "bounces",
  factor_limit = 5,
  categorical_features = categorical_features,
  numeric_features = numeric_features,
  theta = 0.5
)


```

-   This model

```{r}
bounce_model <- bounce_reg[[1]]
ggstats::ggcoef_model(bounce_model,
                       exponentiate = TRUE,
                      conf.int=TRUE) +
  theme_classic(base_size = 6) +
  labs(
    title = "Coeficient Plot for Odds of Purchase Logistic Regression"
  )

ggsave("bounce_model_plot.png",
       plot = last_plot(), 
       width = 8,
       height = 6)
```

```{r}
google_store$bounces %>% table()
```
