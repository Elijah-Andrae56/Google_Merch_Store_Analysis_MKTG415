---
title: "Logistic Regression Final"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r}
library(tidyverse)
library(broom)
library(scales)
library(lubridate)
library(ggplot2)
library(rlang)

google_store <- read_csv("data/google_store.csv")
daily_stats <- read_csv("data/daily_stats.csv")
```

First I will clear the NA values to make the data usable in a Regression

```{r}
google_store <- google_store %>% 
  mutate(
    purchase = ifelse(is.na(revenue) | revenue == 0, 0, 1),
    bounces = replace_na(bounces, 0),
    revenue = replace_na(revenue, 0),
    total_time_on_site = replace_na(total_time_on_site, 0),
    transactions = replace_na(transactions, 0),
    session_quality = replace_na(session_quality, 50)
  )
which(colSums(is.na(google_store)) > 0)
```

Now we will add some extra features to the data. Due to compute times some of
these have been excluded from the file and presaved as a CSV file.
Not every country and holiday is included, however, I tried my
best to include as many as possible using [date.nager
API](https://date.nager.at/api/v3/publicholidays/). View
`HolidayAnalysis.Rmd` for more info on how the holidays were accessed and `days_till_holiday` was calculated.**

```{r}
# Modification Using Lubridate
google_store <- google_store %>%
  mutate(
    date = ymd(google_store$date),
    weekday = wday(date, label = TRUE, abbr =  TRUE),
    is_weekend = wday(date) %in% c(1, 7),
    month = month(date, label = TRUE, abbr = TRUE)
  )

# Joining with a dataset of holidays and days until the next holiday
google_store <- left_join(
  google_store, 
  daily_stats[,c("date", "user_country", "is_holiday", "days_till_holiday")], 
  by=c("date", "user_country")
)

google_store <- google_store %>%
  group_by(fullVisitorId) %>%
  summarize(
    previous_visit_count = n(), previous_purchase_count = sum(purchase)
  ) %>%
  right_join(google_store, 'fullVisitorId')
```



# Finding Optimal Pre-Holiday Marketing Period

I'm thinking there might be an optimal period to market before holidays.

```{r}
holiday_group <- daily_stats %>%
  group_by(days_till_holiday) %>%
  summarize(total_sales_before_holiday = sum(total_sales)) %>%
  arrange(desc(total_sales_before_holiday)) %>%
  na.omit()
```

The most common days to place an order before a holiday are 7, 6, and 4
days. It is useful to market much closer to a holiday then I may have
expected.

```{r}
holiday_plot <- ggplot(holiday_group, aes(x=days_till_holiday, y=total_sales_before_holiday)) +
  geom_line() +
  scale_x_reverse(limits = c(70, 0)) +
  scale_y_continuous(limits = c(0, 600)) +
    labs(
        title='Total Sales vs. Days Till Holiday',
        x='Days Till Holiday',
        y='Total Sales'
    ) +
    theme_minimal()
holiday_plot
```

- Just as I suspected, as it gets closer to a holiday, sales go up around the world. 
- In order to handle this in the logistic regression model, I will apply an inverse transformation. This is because as the number of days gets close to zero generally the odds become higher. With coefficients used in regressions, we need 

```{r}
google_store <- google_store %>%
  left_join(holiday_group, "days_till_holiday")
google_store
```

# Defining Logestic Regression model

-   In this section I will build a logistic regression model build for
    our data set to determine likelihood of a bounce pre visiting, and
    likelihood of a purchase of a user on the site. As a marketer, this
    allows us to determine if it is worthwhile advertising to a user,
    (if they will invest time on the site) based on the way they found
    the site, and purchase likelihood allows us to start preparing for
    an order before it's made, and decide if it's worthwhile to send a
    follow up message if the order is not placed but the user is likely
    to (TP). Or if the user is not worthwhile advertising to in general.

```{r}
logistic_regression <- function(
    df, 
    response_var, 
    factor_limit = 5, 
    numeric_features = NA, 
    categorical_features, 
    theta = 0.5){
  df = na.omit(df)
  # First I need to take an aggressive approach to cleaning the data to avoid 
  # mismatches. For this model I will only consider the top n most common 
  # features in order to adjust speed and accuracy.
  message("Trimming Features")
  # Generally I would define this out of the function. 
  # It's a bit more readable here though.
  top_n_values <- function(column, n) {
    names(head(sort(table(column), decreasing = TRUE), n))}
  
  # Looping through each row and removing categorical
  # features not in the top n most common. This simplifies the dataset to allow
  # reasonable compute times.
  for (col in categorical_features) {  
    top_values <- top_n_values(df[[col]], factor_limit)
    df <- df[df[[col]] %in% top_values, ]}
    #df <- factor(df[[col]], levels = top_values)
  sprintf("The dataset has been reduced to %s rows", nrow(df)) %>% message()
  
  # Need to make sure numeric features isn't NA. 
  # Then standardize them.
  if (!anyNA(numeric_features)){
    for (col in numeric_features){
      df[[col]] <- scale(df[[col]])
    }}
  
  message("Splitting Dataset")
  # Split the dataset into test-train sets
  train_indices <- caret::createDataPartition(df[[response_var]], p = 0.8, list = FALSE)
  
  use_columns <- c(numeric_features, categorical_features, response_var) %>% na.omit()
  
  log_reg_train <- df[train_indices, use_columns]
  log_reg_test <- df[-train_indices, use_columns]
  
  
  message("Building Model")
  
  # Put together model formula
  model_formula <- as.formula(paste(response_var, "~ ."))
  # Build the model
  model <- glm(
    formula = model_formula,
    family = binomial(link = "logit"),
    data = log_reg_train) 
   
  # Predict values for test & train set
  message("Predicting Values")
  # Helper function for simplicity
  predict_values <- function(df, theta=0.5, model){
    df$prediction <- predict(
      model, newdata = df, type = "response")
    df$prediction <- ifelse(df$prediction > theta, 1, 0)
    return(df)}
  
  log_reg_train <- predict_values(log_reg_train, theta, model)
  log_reg_test <- predict_values(log_reg_test, theta, model)
  
  message("Determining results")
  results <- function(df, data_type){
    accuracy <- sum(df[[response_var]] == df$prediction, na.rm=TRUE) / nrow(df)
    confusion_matrix_count <- table(df[[response_var]], df$prediction)
    confusion_matrix <- prop.table(confusion_matrix_count, margin = 1)
    sprintf("Data Type: %s", data_type) %>% print()
    sprintf("The accuracy was %s", percent(accuracy)) %>% print()
    tp <- confusion_matrix[2, 2]
    tn <- confusion_matrix[1, 1]
    sprintf("The TP rate is %s", percent(tp)) %>% print()
    sprintf("The TN rate is %s", percent(tn)) %>% print()
    return(list(accuracy, tp, tn))
  }
  train_results <- results(log_reg_train, "Train")
  test_results <- results(log_reg_test, "Test")
  return(list(model, train_results, test_results))
}
```

```{r}
categorical_features <- c(
  "traffic_source", "traffic_medium", "month", 
  "operating_system", "user_country", "browser")

numeric_features <- c(
  "total_time_on_site", "total_pageviews", "previous_visit_count", 
  "previous_purchase_count", "is_weekend", "total_sales_before_holiday")

purchase_reg <- logistic_regression(
  df = google_store,
  response_var = "purchase",
  factor_limit = 5,
  numeric_features = numeric_features,
  categorical_features = categorical_features,
  theta = 0.05 # Eyeballed optimal theta
)


```
Purchase Model Results:
- Ended up with extremely high accuracy and TN rate. Theta can be adjusted in small increments(+-0.01) to increase TP or NT rate.
- I attempted a ridge regression model. I was able to avoid 0 or 1 probability error, however, I lost the modularity of the model. Did not have time to improve upon.
```{r}
purchase_model <- purchase_reg[[1]]

#ggstats::ggcoef_model(purchase_model,
#                      exponentiate = TRUE,
#                      ci_method="wald")+
ggstatsplot::ggcoefstats(
  x = purchase_model,
  exponentiate = TRUE,
  conf.level = 0.95
)+
  theme_classic(base_size = 5) 
  #scale_x_continuous(limits = c(0, 2))
```
Features to Keep:
- Time on site
- Page views
- 
Remove:
- Traffic Source
 - 'analytics.google.com'
- 
- 'Vietnam'
With these results, I might try to apply this model to a linear
regression plot. With revenue for guessed purchase compared to revenue
for assumed not purchase.

High TN Results: - Reduced marketing spend if targeting ideal potential
buyers. - only focus on visitors more likely to purchase - can exclude
non-buyers from campaigns or personalized offers - Fraud or Bot
Detection. - If bot detection is an issue this can eliminate false
positives and reduce revenue loss for sales that won't go through.

```{r}
categorical_features <- c(
  "traffic_source", "operating_system", "browser", "month")

numeric_features <- c("total_sales_before_holiday", 
                      "previous_visit_count", "previous_purchase_count")

bounce_reg <- logistic_regression(
  df = google_store,
  response_var = "bounces",
  factor_limit = 5,
  categorical_features = categorical_features,
  numeric_features = numeric_features,
  theta = 0.5
)

#tidy(bounce_reg)
```
- This model 

```{r}
bounce_model <- bounce_reg[[1]]
ggstats::ggcoef_model(bounce_model,
                       exponentiate = TRUE) +
  theme_classic(base_size = 5) 
```

```{r}

```




